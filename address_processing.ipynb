{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import codecs\n",
    "import re\n",
    "import csv\n",
    "f = codecs.open(\"bad.csv\", 'r', \"utf_8_sig\")\n",
    "\n",
    "elementaddr = []\n",
    "for line in f:\n",
    "    elementaddr.append(line.strip(\"\\n\"))\n",
    "f.close()\n",
    "\n",
    "\n",
    "mas_id = []\n",
    "mas_address = []\n",
    "\n",
    "for i, line in enumerate(elementaddr):\n",
    "    if i==0:\n",
    "        continue\n",
    "    mas_id.append(line.strip(\"\\n\").split(';')[0])\n",
    "    s = line.strip(\"\\n\").split(';')[1]\n",
    "    s = re.sub(r\"[')№\\\\(,.`<>«»~!@#$%;}{^&*?\\\"|+=_:]\",' ', str(s))\n",
    "    s = s.lower()\n",
    "    s = s.split()\n",
    "    s = ' '.join(s)\n",
    "    mas_address.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function BufferedWriter.close>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import natasha as nat\n",
    "from yargy.tagger import PassTagger\n",
    "from natasha.tokenizer import TOKENIZER as tokenizer\n",
    "tag = PassTagger()\n",
    "\n",
    "def roman_to_arab(txt):\n",
    "    CONV_TABLE = ((1000, 'm'), (900, 'cm'), (500, 'd'), (400, 'cd'),\n",
    "    (100, 'c'), (90, 'xc'), (50, 'l'), (40, 'xl'),\n",
    "    (10, 'x'), (9, 'ix'), (5, 'v'), (4, 'iv'), (1, 'i'))\n",
    "    ret = 0\n",
    "    for arab, roman in CONV_TABLE:\n",
    "        while txt.startswith(roman):\n",
    "            ret += arab\n",
    "            txt = txt[len(roman):]\n",
    "    return ret\n",
    "\n",
    "def Tabl_value(token, table):\n",
    "    f = tablesoperativ[tables.index(table)]\n",
    "    low = 0\n",
    "    high = len(f) - 1\n",
    "    while low <= high:\n",
    "        mid = (low + high) // 2\n",
    "        if token == sorted([token,f[mid]])[0]:\n",
    "            high = mid - 1\n",
    "            actual = low\n",
    "            for i in f[mid].split(' '):\n",
    "                if token == i:\n",
    "                    return(True, actual, len(f[actual]))\n",
    "                \n",
    "        elif token == sorted([token,f[mid]])[1]:\n",
    "            low = mid + 1\n",
    "            actual = high\n",
    "            for i in f[mid].strip(\"\\n\").split(' '):\n",
    "                if token == i:\n",
    "                    return(True, actual, len(f[actual]))\n",
    "        else:\n",
    "            for i in f[mid].strip(\"\\n\").split(' '):\n",
    "                if token == i:\n",
    "                    return(True, index, len(chslov))\n",
    "    \n",
    "    for i in f[actual].strip(\"\\n\").split(' '):\n",
    "        if token == i:\n",
    "            return(True, actual, len(f[actual]))\n",
    "    \n",
    "    return(False, None, None)\n",
    "\n",
    "\n",
    "def Po4_ind(t, token):    \n",
    "    if t.type == 'INT' and len(t.value)==6:\n",
    "        return(True, None, None)\n",
    "    else:\n",
    "        return(False, None, None)\n",
    "\n",
    "def Rim(t, token):\n",
    "    if (t.type == 'LATIN' and ('m' in token or 'c' in token or 'd' in token or 'x' in token or \n",
    "                           'l' in token or 'i' in token or 'v' in token) and \n",
    "    ('a' not in token or 'b' not in token or 'e' not in token or 'f' not in token or 'g' not in token or \n",
    "     'h' not in token or 'j' not in token or 'k' not in token or 'n' not in token or 'o' not in token or \n",
    "     'p' not in token or 'q' not in token or 'r' not in token or 's' not in token or 't' not in token or \n",
    "     'u' not in token or 'w' not in token or 'y' not in token or 'z' not in token)):\n",
    "        return(True, None, None)\n",
    "    else:\n",
    "        return(False, None, None)\n",
    "\n",
    "\n",
    "tables = ['ADROBR.txt','SUBJECT.txt','Settlements.txt','Elements_of_the_road_network.txt',\n",
    "              'Municipalities.txt',\n",
    "              'Elements_of_the_planning_structure.txt',\n",
    "              'Administrative_units.txt', 'Other_items.txt']\n",
    "\n",
    "tablesoperativ = []\n",
    "for table in tables:\n",
    "    f = open('TABLNEW/' + table, 'r')\n",
    "    strings = []\n",
    "    for line in f:\n",
    "        strings.append(line.strip(\"\\n\"))\n",
    "    f.close\n",
    "    tablesoperativ.append(strings)\n",
    "\n",
    "header = [\"id\", \"address\", \"processed address\"]\n",
    "output_file = codecs.open('result_Kazan Artificial Intelligence core.csv', 'w',\"utf_8_sig\")\n",
    "with output_file:\n",
    "    writer = csv.writer(output_file, delimiter=';')\n",
    "    writer.writerow(header)\n",
    "    for q in range(len(mas_address)):\n",
    "        #обработка каждой строки\n",
    "        tokens = tag(tokenizer(mas_address[q])) \n",
    "        #генератор токенов\n",
    "        temp_m = []\n",
    "        temp_txt =[]\n",
    "        for t in tokens:\n",
    "            temp_m.append(t)\n",
    "            temp_txt.append(t.value)\n",
    "\n",
    "        mas = []#список списков тегов - список принадлежностей токена к опр категории (таблице фиас)\n",
    "        mas_nums = []#список списков индексов в каждом теге - попробуем склеивать слова с одинаковыми тегами по этим индексам\n",
    "        mas_lengs = []\n",
    "\n",
    "\n",
    "        for tt, token in enumerate( temp_m):\n",
    "            t_vector = [0] * 9\n",
    "            t_vector_i = [0] * 9\n",
    "            t_vector_l = [0] * 9\n",
    "\n",
    "            for i, table in enumerate(tables):\n",
    "                if i==0:\n",
    "                    t_vector[i], t_vector_i[i], t_vector_l[i] = Po4_ind(token, token.value)#почтовый инжекс\n",
    "                elif i==1:\n",
    "                    t_vector[i], t_vector_i[i], t_vector_l[i] = Rim(token, token.value)#детекор римских цифр\n",
    "                    if t_vector[i] == True:\n",
    "                        break\n",
    "                else:\n",
    "                    t_vector[i], t_vector_i[i], t_vector_l[i] = Tabl_value(token.value, table)\n",
    "                    if t_vector[i] == True:\n",
    "                        break\n",
    "\n",
    "            if t_vector[1] == True:#араб цифры\n",
    "                temp_txt[tt] = roman_to_arab(temp_txt[tt])\n",
    "\n",
    "            t_mas = []\n",
    "            t_mas_i = []\n",
    "            t_mas_l = []\n",
    "            for i, elem in enumerate(t_vector):\n",
    "                if elem == True:\n",
    "                    t_mas.append(i)\n",
    "                    t_mas_i.append(t_vector_i[i])\n",
    "                    t_mas_l.append(t_vector_l[i])\n",
    "\n",
    "            mas.append(t_mas)#сформировали списки, в которые входят токены каждого адреса\n",
    "            mas_nums.append(t_mas_i)#сформировали индексы, списоков в которые входят токены каждого адреса\n",
    "            mas_lengs.append(t_mas_l)\n",
    "\n",
    "        out = ''\n",
    "        for i, word in enumerate(temp_txt):\n",
    "            if mas[i]: # какое условие проверки будет для вывода строки?\n",
    "                out += ' ' +''.join(str(word))\n",
    "        \n",
    "        writer.writerow([mas_id[q].strip(),elementaddr[q+1].strip().split(';')[1],out.strip()])\n",
    "output_file.close"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
